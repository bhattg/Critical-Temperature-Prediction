{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the datasets\n",
    "#### There are 2 datasets.  One contains the physical and chemical properties including the Boiling point, Melting point, Standard Enthalpies, etc. The other file contains the chemical decomposition of the material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train : (17010, 167) the type is <class 'numpy.ndarray'>\n",
      "Shape of Y_train : (17010, 1) the type is <class 'numpy.ndarray'>\n",
      "Shape of X_test :(4253, 167) the type is <class 'numpy.ndarray'>\n",
      "Shape of the Y_test: (4253, 1) the type is <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "train_frame = pd.read_csv(\"train.csv\")\n",
    "train_frame=train_frame.drop(train_frame.columns[train_frame.shape[1]-1], axis=1)\n",
    "train_frame2 = pd.read_csv(\"unique_m.csv\")\n",
    "train_frame=pd.concat([train_frame, train_frame2], axis=1)\n",
    "train_frame=train_frame.drop(train_frame.columns[train_frame.shape[1]-1], axis=1)\n",
    "X_all=train_frame.iloc[:, 0:train_frame.shape[1]-1]\n",
    "Y_all=train_frame.iloc[:, train_frame.shape[1]-1:train_frame.shape[1]]\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(X_all, Y_all, test_size=0.2, random_state=42)\n",
    "X_train=X_train.values\n",
    "Y_train=Y_train.values\n",
    "X_test=X_test.values\n",
    "Y_test=Y_test.values\n",
    "print(\"Shape of X_train : \"+str(X_train.shape)+\" the type is \"+str(type(X_train)))\n",
    "print(\"Shape of Y_train : \"+str(Y_train.shape)+\" the type is \"+str(type(Y_train)))\n",
    "print(\"Shape of X_test :\" +str(X_test.shape)+\" the type is \"+str(type(X_test)))\n",
    "print(\"Shape of the Y_test: \"+str(Y_test.shape)+\" the type is \"+str(type(Y_test)))\n",
    "# #these are arrays now we need to check the outputs for various different classifier.\n",
    "# #note that its a regression problem, depending upon the featurs, we need to predict the value of the Temperature\n",
    "# #Lets Normalize the data first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the input standard to improve learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the data for better learning \n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Support Vector Machine Regressor \n",
    "### Higher the Penalty factor, more the rigid decision boundaries are and increased computation cost on the system. Score metric- R^2 loss.\n",
    "#### The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8547894574946237"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1=svm.SVR(C=30)\n",
    "clf1.fit(X_train_std, Y_train)\n",
    "clf1.score(X_test_std, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Regression Model\n",
    "## Uses squared error loss metric and relu activation in between the layers\n",
    "### Regularised used- Early Stopping. Score metric R^2 loss. \n",
    "#### The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1306: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 173.74580957\n",
      "Validation score: 0.810732\n",
      "Iteration 2, loss = 104.88880099\n",
      "Validation score: 0.843735\n",
      "Iteration 3, loss = 95.15120024\n",
      "Validation score: 0.853921\n",
      "Iteration 4, loss = 88.17802535\n",
      "Validation score: 0.864891\n",
      "Iteration 5, loss = 83.06189378\n",
      "Validation score: 0.871603\n",
      "Iteration 6, loss = 77.34945783\n",
      "Validation score: 0.868033\n",
      "Iteration 7, loss = 74.70103033\n",
      "Validation score: 0.873347\n",
      "Iteration 8, loss = 74.12671053\n",
      "Validation score: 0.872941\n",
      "Iteration 9, loss = 70.89684799\n",
      "Validation score: 0.873124\n",
      "Iteration 10, loss = 70.19262212\n",
      "Validation score: 0.879820\n",
      "Iteration 11, loss = 68.03874526\n",
      "Validation score: 0.883598\n",
      "Iteration 12, loss = 66.54157745\n",
      "Validation score: 0.887163\n",
      "Iteration 13, loss = 63.39973741\n",
      "Validation score: 0.876365\n",
      "Iteration 14, loss = 62.56462945\n",
      "Validation score: 0.882795\n",
      "Iteration 15, loss = 60.65536497\n",
      "Validation score: 0.885462\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size=32, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(500, 100, 20), learning_rate='invscaling',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2=MLPRegressor(batch_size=32, early_stopping=True, verbose=True,learning_rate=\"invscaling\", hidden_layer_sizes=(500,100,20,))\n",
    "clf2.fit(X_train_std, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8879774695782413"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.score(X_test_std, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1306: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 175.05550141\n",
      "Validation score: 0.818540\n",
      "Iteration 2, loss = 105.61352249\n",
      "Validation score: 0.833150\n",
      "Iteration 3, loss = 96.40270102\n",
      "Validation score: 0.847072\n",
      "Iteration 4, loss = 85.43809116\n",
      "Validation score: 0.842014\n",
      "Iteration 5, loss = 81.39629065\n",
      "Validation score: 0.847854\n",
      "Iteration 6, loss = 78.24058413\n",
      "Validation score: 0.863327\n",
      "Iteration 7, loss = 73.86253278\n",
      "Validation score: 0.858489\n",
      "Iteration 8, loss = 72.10419566\n",
      "Validation score: 0.866759\n",
      "Iteration 9, loss = 70.09787502\n",
      "Validation score: 0.872933\n",
      "Iteration 10, loss = 67.38422541\n",
      "Validation score: 0.870045\n",
      "Iteration 11, loss = 66.21732248\n",
      "Validation score: 0.872182\n",
      "Iteration 12, loss = 65.98101825\n",
      "Validation score: 0.874400\n",
      "Iteration 13, loss = 63.18328309\n",
      "Validation score: 0.877872\n",
      "Iteration 14, loss = 62.58062633\n",
      "Validation score: 0.877271\n",
      "Iteration 15, loss = 61.91643628\n",
      "Validation score: 0.877298\n",
      "Iteration 16, loss = 59.73537474\n",
      "Validation score: 0.880298\n",
      "Iteration 17, loss = 59.08020533\n",
      "Validation score: 0.882442\n",
      "Iteration 18, loss = 59.21122824\n",
      "Validation score: 0.888635\n",
      "Iteration 19, loss = 56.90601496\n",
      "Validation score: 0.884965\n",
      "Iteration 20, loss = 56.90835150\n",
      "Validation score: 0.888592\n",
      "Iteration 21, loss = 55.62586420\n",
      "Validation score: 0.887439\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8935364266650208"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3=MLPRegressor(batch_size=32, early_stopping=True, verbose=True,learning_rate=\"adaptive\", hidden_layer_sizes=(500,100,20,))\n",
    "clf3.fit(X_train_std, Y_train)\n",
    "clf3.score(X_test_std, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1306: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 213.84100382\n",
      "Validation score: 0.796523\n",
      "Iteration 2, loss = 107.91713014\n",
      "Validation score: 0.818604\n",
      "Iteration 3, loss = 97.82378075\n",
      "Validation score: 0.826815\n",
      "Iteration 4, loss = 89.09075550\n",
      "Validation score: 0.843979\n",
      "Iteration 5, loss = 83.92387188\n",
      "Validation score: 0.850580\n",
      "Iteration 6, loss = 79.10142948\n",
      "Validation score: 0.854356\n",
      "Iteration 7, loss = 75.34382585\n",
      "Validation score: 0.851999\n",
      "Iteration 8, loss = 74.76930055\n",
      "Validation score: 0.867361\n",
      "Iteration 9, loss = 70.40918615\n",
      "Validation score: 0.856065\n",
      "Iteration 10, loss = 69.32189181\n",
      "Validation score: 0.865879\n",
      "Iteration 11, loss = 67.24946586\n",
      "Validation score: 0.848532\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8758956268619689"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4=MLPRegressor(batch_size=64, early_stopping=True, verbose=True,learning_rate=\"adaptive\", hidden_layer_sizes=(500,100,20,))\n",
    "clf4.fit(X_train_std, Y_train)\n",
    "clf4.score(X_test_std, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1306: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 159.16750993\n",
      "Validation score: 0.821094\n",
      "Iteration 2, loss = 101.92259746\n",
      "Validation score: 0.844458\n",
      "Iteration 3, loss = 92.71528641\n",
      "Validation score: 0.851593\n",
      "Iteration 4, loss = 86.28530578\n",
      "Validation score: 0.845682\n",
      "Iteration 5, loss = 82.52380637\n",
      "Validation score: 0.851729\n",
      "Iteration 6, loss = 76.85495881\n",
      "Validation score: 0.858882\n",
      "Iteration 7, loss = 74.93437903\n",
      "Validation score: 0.824006\n",
      "Iteration 8, loss = 72.75177619\n",
      "Validation score: 0.864155\n",
      "Iteration 9, loss = 70.84384786\n",
      "Validation score: 0.870486\n",
      "Iteration 10, loss = 68.29285459\n",
      "Validation score: 0.875336\n",
      "Iteration 11, loss = 65.98404374\n",
      "Validation score: 0.872384\n",
      "Iteration 12, loss = 64.46145694\n",
      "Validation score: 0.867741\n",
      "Iteration 13, loss = 64.12483146\n",
      "Validation score: 0.879985\n",
      "Iteration 14, loss = 63.05516240\n",
      "Validation score: 0.885587\n",
      "Iteration 15, loss = 60.77034863\n",
      "Validation score: 0.884495\n",
      "Iteration 16, loss = 59.81281801\n",
      "Validation score: 0.883296\n",
      "Iteration 17, loss = 58.05872028\n",
      "Validation score: 0.885023\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8910338101702322"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf7=MLPRegressor(batch_size=32, early_stopping=True, verbose=True,learning_rate=\"adaptive\", hidden_layer_sizes=(500,100,20,))\n",
    "clf7.fit(X_train_std, Y_train)\n",
    "clf7.score(X_test_std, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1306: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 165.69187342\n",
      "Iteration 2, loss = 104.49895384\n",
      "Iteration 3, loss = 92.47038400\n",
      "Iteration 4, loss = 86.22899868\n",
      "Iteration 5, loss = 81.70295475\n",
      "Iteration 6, loss = 77.14981213\n",
      "Iteration 7, loss = 74.06432963\n",
      "Iteration 8, loss = 70.92371556\n",
      "Iteration 9, loss = 70.06862639\n",
      "Iteration 10, loss = 66.95599309\n",
      "Iteration 11, loss = 65.62439144\n",
      "Iteration 12, loss = 65.01318693\n",
      "Iteration 13, loss = 61.96536594\n",
      "Iteration 14, loss = 62.35832564\n",
      "Iteration 15, loss = 59.41693334\n",
      "Iteration 16, loss = 59.57695405\n",
      "Iteration 17, loss = 58.61965290\n",
      "Iteration 18, loss = 56.31074944\n",
      "Iteration 19, loss = 54.86195276\n",
      "Iteration 20, loss = 54.33171126\n",
      "Iteration 21, loss = 54.14790112\n",
      "Iteration 22, loss = 53.38640222\n",
      "Iteration 23, loss = 52.36690918\n",
      "Iteration 24, loss = 51.00426875\n",
      "Iteration 25, loss = 50.45049084\n",
      "Iteration 26, loss = 49.31639210\n",
      "Iteration 27, loss = 49.36544533\n",
      "Iteration 28, loss = 48.51796611\n",
      "Iteration 29, loss = 48.54640048\n",
      "Iteration 30, loss = 47.48097502\n",
      "Iteration 31, loss = 47.21900910\n",
      "Iteration 32, loss = 46.49107906\n",
      "Iteration 33, loss = 45.23098802\n",
      "Iteration 34, loss = 45.14114594\n",
      "Iteration 35, loss = 44.47652033\n",
      "Iteration 36, loss = 44.49305587\n",
      "Iteration 37, loss = 43.15092473\n",
      "Iteration 38, loss = 43.37467149\n",
      "Iteration 39, loss = 42.30552615\n",
      "Iteration 40, loss = 42.39314563\n",
      "Iteration 41, loss = 42.98982416\n",
      "Iteration 42, loss = 41.54530176\n",
      "Iteration 43, loss = 41.04937825\n",
      "Iteration 44, loss = 40.60107194\n",
      "Iteration 45, loss = 40.80810903\n",
      "Iteration 46, loss = 40.26290914\n",
      "Iteration 47, loss = 39.12015080\n",
      "Iteration 48, loss = 39.64376340\n",
      "Iteration 49, loss = 39.28917028\n",
      "Iteration 50, loss = 39.09263799\n",
      "Iteration 51, loss = 39.04107908\n",
      "Iteration 52, loss = 38.81833064\n",
      "Iteration 53, loss = 37.97174802\n",
      "Iteration 54, loss = 37.22909379\n",
      "Iteration 55, loss = 37.31982614\n",
      "Iteration 56, loss = 37.09393098\n",
      "Iteration 57, loss = 36.80066895\n",
      "Iteration 58, loss = 36.33871062\n",
      "Iteration 59, loss = 36.86942363\n",
      "Iteration 60, loss = 36.54303777\n",
      "Iteration 61, loss = 35.67429958\n",
      "Iteration 62, loss = 35.42916008\n",
      "Iteration 63, loss = 35.57081845\n",
      "Iteration 64, loss = 35.67409963\n",
      "Iteration 65, loss = 34.82929275\n",
      "Iteration 66, loss = 35.03686476\n",
      "Iteration 67, loss = 34.90597445\n",
      "Iteration 68, loss = 34.67983432\n",
      "Iteration 69, loss = 34.55843668\n",
      "Iteration 70, loss = 34.30617601\n",
      "Iteration 71, loss = 34.17755735\n",
      "Iteration 72, loss = 33.96949252\n",
      "Iteration 73, loss = 33.19527395\n",
      "Iteration 74, loss = 33.24340430\n",
      "Iteration 75, loss = 34.00201621\n",
      "Iteration 76, loss = 33.53719420\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9179005127483747"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf8=MLPRegressor(batch_size=32, early_stopping=False, verbose=True,learning_rate=\"invscaling\", hidden_layer_sizes=(500,100,20,))\n",
    "clf8.fit(X_train_std, Y_train)\n",
    "clf8.score(X_test_std, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1306: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 171.46322826\n",
      "Validation score: 0.792167\n",
      "Iteration 2, loss = 104.91485516\n",
      "Validation score: 0.833154\n",
      "Iteration 3, loss = 94.33602053\n",
      "Validation score: 0.847224\n",
      "Iteration 4, loss = 89.45549020\n",
      "Validation score: 0.837888\n",
      "Iteration 5, loss = 84.16378131\n",
      "Validation score: 0.858469\n",
      "Iteration 6, loss = 77.57434386\n",
      "Validation score: 0.854015\n",
      "Iteration 7, loss = 75.19996180\n",
      "Validation score: 0.858919\n",
      "Iteration 8, loss = 72.66200981\n",
      "Validation score: 0.874206\n",
      "Iteration 9, loss = 71.90231266\n",
      "Validation score: 0.868283\n",
      "Iteration 10, loss = 69.28922541\n",
      "Validation score: 0.874431\n",
      "Iteration 11, loss = 66.92032488\n",
      "Validation score: 0.878654\n",
      "Iteration 12, loss = 64.83989741\n",
      "Validation score: 0.882034\n",
      "Iteration 13, loss = 63.79111416\n",
      "Validation score: 0.877192\n",
      "Iteration 14, loss = 64.59042496\n",
      "Validation score: 0.889599\n",
      "Iteration 15, loss = 60.90144059\n",
      "Validation score: 0.883335\n",
      "Iteration 16, loss = 60.38007878\n",
      "Validation score: 0.884716\n",
      "Iteration 17, loss = 58.70327924\n",
      "Validation score: 0.892270\n",
      "Iteration 18, loss = 58.32809231\n",
      "Validation score: 0.884718\n",
      "Iteration 19, loss = 56.35421647\n",
      "Validation score: 0.894980\n",
      "Iteration 20, loss = 55.88720724\n",
      "Validation score: 0.889726\n",
      "Iteration 21, loss = 55.11188636\n",
      "Validation score: 0.896182\n",
      "Iteration 22, loss = 54.04008472\n",
      "Validation score: 0.896323\n",
      "Iteration 23, loss = 53.42382265\n",
      "Validation score: 0.899033\n",
      "Iteration 24, loss = 52.55972462\n",
      "Validation score: 0.894946\n",
      "Iteration 25, loss = 51.64560734\n",
      "Validation score: 0.905252\n",
      "Iteration 26, loss = 50.57015969\n",
      "Validation score: 0.887185\n",
      "Iteration 27, loss = 50.51777727\n",
      "Validation score: 0.896250\n",
      "Iteration 28, loss = 49.07458077\n",
      "Validation score: 0.904554\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8986828361103292"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf9=MLPRegressor(batch_size=32, early_stopping=True, verbose=True,learning_rate=\"invscaling\", hidden_layer_sizes=(500,100,20,), max_iter=1000)\n",
    "clf9.fit(X_train_std, Y_train)\n",
    "clf9.score(X_test_std, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1306: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 173.40242690\n",
      "Iteration 2, loss = 101.71611983\n",
      "Iteration 3, loss = 92.32044854\n",
      "Iteration 4, loss = 85.18434040\n",
      "Iteration 5, loss = 80.98216878\n",
      "Iteration 6, loss = 75.70970200\n",
      "Iteration 7, loss = 73.15589379\n",
      "Iteration 8, loss = 71.46387221\n",
      "Iteration 9, loss = 70.90720522\n",
      "Iteration 10, loss = 67.95617032\n",
      "Iteration 11, loss = 65.46792258\n",
      "Iteration 12, loss = 63.72137064\n",
      "Iteration 13, loss = 61.85805928\n",
      "Iteration 14, loss = 61.03213098\n",
      "Iteration 15, loss = 60.09319157\n",
      "Iteration 16, loss = 58.90535863\n",
      "Iteration 17, loss = 57.71707483\n",
      "Iteration 18, loss = 56.26515009\n",
      "Iteration 19, loss = 55.53939701\n",
      "Iteration 20, loss = 53.61214691\n",
      "Iteration 21, loss = 52.51564420\n",
      "Iteration 22, loss = 52.98059167\n",
      "Iteration 23, loss = 51.12332691\n",
      "Iteration 24, loss = 49.79314687\n",
      "Iteration 25, loss = 48.73416962\n",
      "Iteration 26, loss = 48.53011733\n",
      "Iteration 27, loss = 47.36702669\n",
      "Iteration 28, loss = 45.97248545\n",
      "Iteration 29, loss = 46.59965976\n",
      "Iteration 30, loss = 45.12841258\n",
      "Iteration 31, loss = 44.68209663\n",
      "Iteration 32, loss = 44.20362191\n",
      "Iteration 33, loss = 43.24577716\n",
      "Iteration 34, loss = 43.22353747\n",
      "Iteration 35, loss = 42.81626617\n",
      "Iteration 36, loss = 43.01824661\n",
      "Iteration 37, loss = 42.02899639\n",
      "Iteration 38, loss = 41.49871926\n",
      "Iteration 39, loss = 41.87763501\n",
      "Iteration 40, loss = 40.73134346\n",
      "Iteration 41, loss = 41.39406974\n",
      "Iteration 42, loss = 40.39710134\n",
      "Iteration 43, loss = 40.11664922\n",
      "Iteration 44, loss = 39.67511607\n",
      "Iteration 45, loss = 39.40502148\n",
      "Iteration 46, loss = 39.28695490\n",
      "Iteration 47, loss = 38.65365575\n",
      "Iteration 48, loss = 37.96106741\n",
      "Iteration 49, loss = 38.11787715\n",
      "Iteration 50, loss = 37.97355641\n",
      "Iteration 51, loss = 37.84986723\n",
      "Iteration 52, loss = 37.75619206\n",
      "Iteration 53, loss = 36.80192987\n",
      "Iteration 54, loss = 37.13019013\n",
      "Iteration 55, loss = 36.77347709\n",
      "Iteration 56, loss = 36.47244235\n",
      "Iteration 57, loss = 36.12688030\n",
      "Iteration 58, loss = 36.67190639\n",
      "Iteration 59, loss = 36.09849732\n",
      "Iteration 60, loss = 35.60167869\n",
      "Iteration 61, loss = 34.95048202\n",
      "Iteration 62, loss = 34.75168886\n",
      "Iteration 63, loss = 36.25844393\n",
      "Iteration 64, loss = 35.44127391\n",
      "Iteration 65, loss = 34.63758363\n",
      "Iteration 66, loss = 33.84718149\n",
      "Iteration 67, loss = 34.46772993\n",
      "Iteration 68, loss = 33.95549921\n",
      "Iteration 69, loss = 34.78866497\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9150003154331632"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf10=MLPRegressor(batch_size=32, early_stopping=False, verbose=True,learning_rate=\"adaptive\", hidden_layer_sizes=(500,100,20,))\n",
    "clf10.fit(X_train_std, Y_train)\n",
    "clf10.score(X_test_std, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As results, we get the maximum score of 0.8986 with early stopping criteria of 10% of maximum iterations on 10% of training set. \n",
    "#### Without Early stopping we achieved the score of 0.915. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
